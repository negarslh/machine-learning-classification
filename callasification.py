# -*- coding: utf-8 -*-
"""project_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1laU0RpA-2oNP2zfMxM4XR5t4ZCm_mYgY

## Decision Tree

The goal of this project is to train and test a decision tree classifier on 'breast cancer' dataset.

what you should do:
- answere to â€Œ4 questions, by writing the appropriate code.
"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import numpy as np

"""### Reading the "breast cancer" dataset from sklearn"""

bcancer = load_breast_cancer()
df = pd.DataFrame(np.c_[bcancer['data'], bcancer['target']],
                  columns= np.append(bcancer['feature_names'], ['target']))
df

print('the number of instances in database with cancer is {}'.format(df.loc[df.target==1].shape[0]))
print('the number of instances in database without cancer is {}'.format(df.loc[df.target==0].shape[0]))

"""Steps we are going to take in this tutorial:

- buiding a decision tree model on "train set":
  - information gain
  - gini index
- testing the model on "test set", and calculating the 1) accuracy, 2) precision and recall, and F-measure
  - holdout
  - cross-validation
- pre-prunning the decision tree by:
  - limiting the deapth of the tree
  - limiting the minimum number of samples required to split an node.

### prepare data:
"""

y = df['target']
X = df.drop(columns=['target'])

"""### Holdout method : randomly partition the dataset into two independent sets - train set and test set"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

"""### train the decision tree cosidering the "gini index" as the spliting criterion:"""

clf_gini = DecisionTreeClassifier(criterion='gini')
clf_gini.fit(X_train, y_train)

"""### using the trained model to predict the class label of the instances in test set, and calculating the accuracy of the model:"""

from sklearn.metrics import accuracy_score

y_pred_gini = clf_gini.predict(X_test)

print(y_pred_gini[:10])

gini_acc = accuracy_score(y_test, y_pred_gini)
print('Model accuracy score with criterion gini index: {0:0.4f}'. format(gini_acc))

"""### question 1:
- plot the "confusion matrix",
- calculate "Precision", "Recall", and "F-score"

**gini index tree**
"""

from sklearn.tree import plot_tree
plt.figure(figsize=(10,8), dpi=150)
plot_tree(clf_gini, feature_names=X.columns, filled=True);

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

"""**gini accuracy**"""

gini_acc = accuracy_score(y_test, y_pred_gini)
print('Model accuracy score with criterion gini index: {0:0.4f}'. format(gini_acc))

"""**gini precision**"""

gini_precision = precision_score(y_test, y_pred_gini)
print('Model precision score with criterion gini index: {0:0.4f}'. format(gini_precision))

"""**gini Recall**"""

gini_recall = recall_score(y_test, y_pred_gini)
print('Model recall score with criterion gini index: {0:0.4f}'. format(gini_recall))

"""**gini F-measure**"""

gini_f_score = f1_score(y_test, y_pred_gini)
print('Model f-measure score with criterion gini index: {0:0.4f}'. format(gini_f_score))

"""**Confusion Matrix**"""

from sklearn.metrics import ConfusionMatrixDisplay


classifier = clf_gini.fit(X_train, y_train)

titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        cmap=plt.cm.cool,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()



"""### plot the decision tree:"""

import matplotlib.pyplot as plt
from sklearn import tree

plt.figure(figsize=(20,10))
tree.plot_tree(clf_gini.fit(X_train, y_train))

"""### question 2:
- train the model using "information gain"
- test the model and calculate all the measures (accuracy, precision, recall, f-score)
- plot the tree
"""

# write the code here

from sklearn import tree

clf_entropy = tree.DecisionTreeClassifier(criterion='entropy')
clf_entropy.fit(X_train, y_train)

clf_entropy.score(X, y)

y_pred_entropy = clf_entropy.predict(X_test)

print(y_pred_entropy[:10])

"""**Accuracy Score**

acc = (tp+tn) / (tp+tn+fp+fn)
"""

entropy_acc = accuracy_score(y_test, y_pred_entropy)
print('Model accuracy score with criterion information gain: {0:0.4f}'. format(entropy_acc))

"""**Precision Score**

precision = tp / tp+fp
"""

entropy_precision = precision_score(y_test, y_pred_entropy)
print('Model precision score with criterion information gain: {0:0.4f}'. format(entropy_precision))

"""**Recall Score**

recall = tp / tp+fn
"""

entropy_recall = recall_score(y_test, y_pred_entropy)
print('Model recall score with criterion information gain: {0:0.4f}'. format(entropy_recall))

"""**F-Score**

f-measure = (2*recall*precision) / (recall+precision)
"""

entropy_f_measure = f1_score(y_test, y_pred_entropy)
print('Model f-measure score with criterion information gain: {0:0.4f}'. format(entropy_f_measure))

classifier = clf_entropy.fit(X_train, y_train)

titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        cmap=plt.cm.RdPu,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()

"""**information gain tree**"""

from sklearn.tree import plot_tree
plt.figure(figsize=(10,8), dpi=150)
plot_tree(clf_entropy, feature_names=X.columns, filled=True);

plt.figure(figsize=(20,10))
tree.plot_tree(clf_entropy.fit(X_train, y_train))

"""### question 3: train and test the model using '10-fold cross validation'
- with both considering 'gini index' and 'information gain'

**Information Gain**
"""

from sklearn.model_selection import cross_val_score,KFold

kf=KFold(n_splits=10) #create 10-fold
score=cross_val_score(clf_entropy,X,y,cv=kf) #cross validation score

print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

"""**Gini Index**"""

kf=KFold(n_splits=10) #create 10-fold
score=cross_val_score(clf_gini,X,y,cv=kf) #cross validation score

print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

"""### question 4: apply pre-prunning by:
- limiting the deapth of the tree
- limiting the minimum number of samples required to split an node.

**Gini Index**
"""

clf_gini = DecisionTreeClassifier(criterion='gini',max_depth= 17,min_samples_split= 12,splitter= 'random')
clf_gini.fit(X_train, y_train)
plt.figure(figsize=(20,12))
tree.plot_tree(clf_gini,rounded=True,filled=True)
plt.show()

"""**Informaition Gain**"""

clf_entropy = DecisionTreeClassifier(criterion='entropy',max_depth= 17,min_samples_split= 12,splitter= 'random')
clf_entropy.fit(X_train, y_train)
plt.figure(figsize=(20,12))
tree.plot_tree(clf_entropy,rounded=True,filled=True)
plt.show()